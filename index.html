<!DOCTYPE html>
<html>
	<head>
		<title>Speech Recognition JS using WebAudio</title>
		<script src="js/resampler.js"></script>
		<script src="js/talk.js"></script>
		<script src="js/recorder.js"></script>
		<link rel="stylesheet" type="text/css" href="css/style.css"/>
	</head>
	<body>
		<h1>Speech Recognition JS</h1>
		<p id="info-box">This demo uses <a href="http://updates.html5rocks.com/2012/09/Live-Web-Audio-Input-Enabled">WebRTC getUserMedia API</a> for live microphone input. For the <a href="http://code.google.com/p/wami-recorder" target="_blank">WAMI</a> (Flash plugin required) version check the demo <a href="wami.html">here</a> (works in all browsers)</p>
		<h2>Configuration</h2>
		<div id="info-box">
			Language and Acoustic model parameters:<br/>
			<select id="sel-model">
				<option value="wsj">Select Model:</option>
				<option value="digits">Digits</option>
				<option value="digits_fsg">Digits (FSG)</option>
				<option value="turtle">Turtle</option>
				<option value="wsj">WSJ</option>
			</select>
			<select id="sel-samprate">
				<option value="16000">Sample Rate:</option>
				<option value="8000">8,000 Hz</option>
				<option value="16000">16,000 Hz</option>
			</select>
			<br/>
			<hr/>
			N-best hypotheses:
			<input type="number" id="sel-nbest"
				min="0"
				max="10"
				step="1"
				value="3">
			</div>
			<h2>Testing</h2>
			<div id="info-box">
			<div id="test-files">
			Test recorded RAW audio:
				<select id="sel-test">
					<option value="none">Select testcase:</option>
					<option value="digits">numbers.raw - Digits</option>
					<option value="digits_fsg">numbers.raw - Digits (FSG)</option>
					<option value="turtle">goforward.raw - Turtle</option>
				</select><br/>
				<audio controls="controls" id="play-default-test" src="" type="audio/wav">
					<embed height="50" width="100" src="">
				</audio>

			</div>
			<hr/>
			<div id="test-upload">
			Upload your own RAW/WAV (16bit PCM, little-endian), select corresponding <i>sample rate</i> (only 8kHz and 16kHz recordings supported due to the restrictions of the acoustic model) from <b>Configuration</b> and click <em>Decode Audio</em>:<br/>

			<input type="file" id="raw_audio" name="file" />
			<button onclick='processing();readRawAudio();'>Decode Audio</button>
			<audio controls="controls" id="play-upload" src="" type="audio/wav" autoplay>
				<embed height="50" width="100" src="">
			</audio>
			</div>
			<hr/>
				<div>Live microphone input (using WebRTC) -- enable <b>Web Audio Input</b> in chrome://flags <br/>
				Select <i>model</i> from <b>Configuration</b> (<i>sample rate</i> is set by AudioContext object), click <em>Record Audio</em>, speak, and <em>Stop | Decode</em>
				<div id='model_config'>
					Model: <b><span id="m_conf"></span></b><br/>
					Sample Rate: <b><span id="sr_conf"></span></b></br>
					<span id='download_rec'></span>
				</div>
			</div>
			<div>
				<button onclick='processing();recordAudio();'>Record Audio</button>
				<button onclick='stopRecording();'>Stop | Decode</button>
				<audio controls="controls" id="play-rec" src="" type="audio/wav" autoplay>
					<embed height="50" width="100" src="">
				</audio>
			</div>

		<div id="srec_hyp"></div>
		<div id="srec_hyp_nbest"></div>
		<pre><div id="srec_timer"></div></pre>
		</div>

		<div id="status"></div>

		<script>
		var PATH = window.location.href.replace(/\/\w+.html/g, '/');
		var sel_model = document.getElementById("sel-model");
		var sel_samprate = document.getElementById("sel-samprate");
		document.getElementById("sr_conf").innerHTML = sel_samprate.options[2].text; // hardcoded default
		document.getElementById("m_conf").innerHTML = sel_model.options[4].text; // hardcoded default
		var configDigits = {};
		configDigits.hmmDir = PATH + "model/hmm/en/tidigits";
		configDigits.dmp = PATH + "model/lm/en/tidigits.DMP";
		configDigits.dic = PATH + "model/lm/en/tidigits.dic";

		var configDigitsFSG = {};
		configDigitsFSG.hmmDir = PATH + "model/hmm/en/tidigits";
		configDigitsFSG.dic = PATH + "model/lm/en/tidigits.dic";
		configDigitsFSG.fsg = PATH + "model/lm/en/tidigits.fsg";

		var configTurtle = {};
		configTurtle.hmmDir = PATH + "model/hmm/en_US/hub4wsj_sc_8k";
		configTurtle.dmp = PATH + "model/lm/en/turtle.DMP";
		configTurtle.dic = PATH + "model/lm/en/turtle.dic";

		var configWSJ = {};
		configWSJ.hmmDir = PATH + "model/hmm/en_US/hub4wsj_sc_8k";
		configWSJ.dmp = PATH + "model/lm/en_US/hub4.5000.DMP";
		configWSJ.dic = PATH + "model/lm/en_US/cmu07a.dic";

		var configDic = {
			turtle: configTurtle,
			digits: configDigits,
			digits_fsg: configDigitsFSG,
			wsj: configWSJ
		};

		sel_model.onchange = function() {
			var i = sel_model.selectedIndex;
			document.getElementById("m_conf").innerHTML = sel_model.options[i].text;
		}
		sel_samprate.onchange = function() {
			var i = sel_samprate.selectedIndex;
			document.getElementById("sr_conf").innerHTML = sel_samprate.options[i].text;
		}

		var processing = function() {
			document.getElementById("srec_hyp").innerHTML = "<pre>processing ... </pre>";
			document.getElementById("srec_timer").innerHTML = "";
			document.getElementById("srec_hyp_nbest").innerHTML = "";
		};

		var showDecodingResult = function(talkResult) {
			// Get hypothesis
			if (talkResult.hypothesis) {
				document.getElementById("srec_hyp").innerHTML=talkResult.convertDigits();
			}
			// Get N-best hypotheses
			if (talkResult.nbest) {
			for (var i = 0; i < talkResult.nbest.length; i++) {
			  document.getElementById("srec_hyp_nbest").innerHTML += talkResult.nbest[i] + "<br/>";
			};
			}
			// Get timer output
			if (talkResult.timer) {
			document.getElementById("srec_timer").innerHTML="TIME: "+ talkResult.timer.toString();
			}
		};

		document.getElementById("sel-test").onchange = function() {
			processing();
			configDigits.nbest = document.getElementById("sel-nbest").value;
			configDigitsFSG.nbest = document.getElementById("sel-nbest").value;
			configTurtle.nbest = document.getElementById("sel-nbest").value;
			switch(this.value) {
				case "digits":
				recognizeSpeech(PATH + "test/data/numbers.raw", configDigits, showDecodingResult, true);
				enableAudioPlayback("play-default-test", "test/data/numbers.wav");
				break;
				case "digits_fsg":
				recognizeSpeech(PATH + "test/data/numbers.raw", configDigitsFSG, showDecodingResult, true);
				enableAudioPlayback("play-default-test", "test/data/numbers.wav");
				break;
				case "turtle":
				recognizeSpeech(PATH + "test/data/goforward.raw", configTurtle, showDecodingResult, true);
				enableAudioPlayback("play-default-test", "test/data/goforward.wav");
				break;
				default:
				document.getElementById("srec_hyp").innerHTML = "Select a testcase from the drop menu";
			}
		};

		function enableAudioPlayback(audioID, audioSRC) {
			var audioObj = document.getElementById(audioID);
			audioObj.src = audioSRC;
			audioObj.children[0].src = audioSRC; // fallback if no HTML5
			audioObj.style.display = "block";
			// enable download
			var downloadLink = document.getElementById("download_rec");
			downloadLink.innerHTML = "Download decoded <a href='"+audioSRC+"' download='audio.wav'>audio.wav</a>";
		}

		function readRawAudio() {
		    var files = document.getElementById('raw_audio').files;
		    if (!files.length) {
		      alert('Please select a RAW/WAV audio file!');
		      return;
		    }

		    var file = files[0];
		    var start = 0;
		    var stop = file.size - 1;

		    var reader = new FileReader();

		    // If we use onloadend, we need to check the readyState.
		    reader.onloadend = function(evt) {
		      if (evt.target.readyState == FileReader.DONE) { // DONE == 2
		      	// ---- Recognize audio using talk.js ----
		      	var crntConfig = configDic[document.getElementById("sel-model").value];
		      	crntConfig.sampleRate = document.getElementById("sel-samprate").value;
		      	crntConfig.nbest = document.getElementById("sel-nbest").value;
				recognizeSpeech(evt.target.result, crntConfig, showDecodingResult);
		      }
		    };

		    var blob = file.slice(start, stop + 1);
		    // Play uploaded audio
			var url = (window.URL || window.webkitURL).createObjectURL(blob);
			enableAudioPlayback("play-upload", url);

		    reader.readAsBinaryString(blob);
		}

		// -------------- AUDIO RECORDING -----------------
		var audioRecorder;
    var audioStreamPromise;
		var context;
		// Check for webRTC compatibility
		try {
			window.AudioContext = window.AudioContext || window.webkitAudioContext;
			context = new AudioContext();
			navigator.mediaDevices.getUserMedia({audio:true}).then(function(stream) {
        console.log('GOT STREAM');
        var mediaStreamSource = context.createMediaStreamSource(stream);
        // Record only MONO since PocketSphinx decoder doesn't parse stereo very well
        audioRecorder = new Recorder(mediaStreamSource, {channelCount: 1});
      });
		} catch(e) {
		   alert('Web Audio API is not supported in this browser');
		}

		function recordAudio() {
			document.getElementById("srec_hyp").innerHTML = "<pre>Recording ... </pre>";
			audioRecorder.record();
		}

		function stopRecording() {
			processing();
			var toSampleRate = document.getElementById("sel-samprate").value;
			audioRecorder.getWAV(gotWAVData, 'audio/wav', toSampleRate);
			//audioRecorder.getWAV(gotWAVData);
			//audioRecorder.getBuffer(gotData);
			audioRecorder.stop();
		}
		function resampleAudio(buffer, fromSampleRate, toSampleRate) {
			var bufferSize = Math.ceil(buffer.length * toSampleRate/fromSampleRate);
			var resamplerControl = new Resampler(fromSampleRate, toSampleRate, 2, bufferSize, true);
			var resampleLength = resamplerControl.resampler(buffer);
			return resamplerControl.outputBuffer;
		}

		function gotWAVData(blob) {
			audioRecorder.clear();
			// Play audio recording
			var url = (window.URL || window.webkitURL).createObjectURL(blob);
			enableAudioPlayback("play-rec", url);

			var crntConfig = configDic[document.getElementById("sel-model").value];
			crntConfig.sampleRate = document.getElementById("sel-samprate").value;
			crntConfig.nbest = document.getElementById("sel-nbest").value;
		    var reader = new FileReader();

		    reader.onloadend = function(evt) {
		      if (evt.target.readyState == FileReader.DONE) { // DONE == 2
				recognizeSpeech(evt.target.result, crntConfig, showDecodingResult);
		      }
		    };
		    //Recorder.downloadWAV(blob);
		    reader.readAsBinaryString(blob);

		}
		// Callback function for getBuffer from recorder.js
		function gotData(blob) {
			audioRecorder.clear();
			// Resample buffer -------------------------------------
			var origSampleRate = context.sampleRate;
			var toSampleRate = document.getElementById("sel-samprate").value;
			var resampledBuffer = resampleAudio(blob, origSampleRate, toSampleRate);
			var dataview = encodeWAV(resampledBuffer, toSampleRate);
			var audioBlob = new Blob([dataview], { type: 'audio/wav' });
			blob = audioBlob;
			console.log("[RESAMPLING] From "+origSampleRate+"Hz to "+toSampleRate+"Hz");
			// -----------------------------------------------------

			// Play audio recording
			var url = (window.URL || window.webkitURL).createObjectURL(blob);
			enableAudioPlayback("play-rec", url);

			var crntConfig = configDic[document.getElementById("sel-model").value];
			crntConfig.sampleRate = toSampleRate;
			crntConfig.nbest = document.getElementById("sel-nbest").value;
		    var reader = new FileReader();

		    reader.onloadend = function(evt) {
		      if (evt.target.readyState == FileReader.DONE) { // DONE == 2
				recognizeSpeech(evt.target.result, crntConfig, showDecodingResult);
		      }
		    };
		    Recorder.downloadWAV(blob);
		    reader.readAsBinaryString(blob);

		}
		</script>
	</body>
</html>
